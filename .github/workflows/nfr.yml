name: NFR Tests

on:
  pull_request:
    branches: [develop, main]

permissions:
  contents: read
  pull-requests: write

jobs:
  nfr:
    name: NFR Scenarios
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'
          cache: true

      - name: Download dependencies
        run: go mod download

      - name: Run NFR Tests
        id: nfr
        run: |
          # Run NFR tests
          set +e
          go test -v -run TestNFRScenarios ./internal/mcp/... 2>&1 | tee nfr-output.txt
          TEST_EXIT=${PIPESTATUS[0]}
          set -e

          # Parse results (remove ^ anchor to count subtests too)
          echo "## ğŸ“Š NFR Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          PASS=$(grep -c -- "--- PASS" nfr-output.txt || echo "0")
          FAIL=$(grep -c -- "--- FAIL" nfr-output.txt || echo "0")
          TOTAL=$((PASS + FAIL))

          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| âœ… Passed | $PASS |" >> $GITHUB_STEP_SUMMARY
          echo "| âŒ Failed | $FAIL |" >> $GITHUB_STEP_SUMMARY
          echo "| ğŸ“Š Total | $TOTAL |" >> $GITHUB_STEP_SUMMARY

          # Set outputs for PR comment
          echo "pass=$PASS" >> $GITHUB_OUTPUT
          echo "fail=$FAIL" >> $GITHUB_OUTPUT
          echo "total=$TOTAL" >> $GITHUB_OUTPUT
          echo "exit_code=$TEST_EXIT" >> $GITHUB_OUTPUT

          exit $TEST_EXIT

      - name: Parse NFR metrics
        id: metrics
        if: always()
        run: |
          # Extract actual token counts and timings from test output
          # Format: tool_tier_actual_baseline_status

          cat > parse_nfr.py << 'EOF'
          import re

          results = []

          with open("nfr-output.txt", "r") as f:
              content = f.read()

          # Parse: "searchSymbols_small: 3500 bytes (baseline: 3600, max: 3960)"
          pattern = r'(\w+)_(\w+): (\d+) bytes \(baseline: (\d+), max: (\d+)\)'
          for match in re.finditer(pattern, content):
              tool, tier, actual, baseline, max_allowed = match.groups()
              actual, baseline = int(actual), int(baseline)
              diff_pct = ((actual - baseline) / baseline) * 100

              if diff_pct <= -5:
                  status = "ğŸŸ¢"
                  trend = f"â†“{abs(diff_pct):.1f}%"
              elif diff_pct >= 10:
                  status = "ğŸ”´"
                  trend = f"â†‘{diff_pct:.1f}%"
              elif diff_pct >= 5:
                  status = "ğŸŸ¡"
                  trend = f"â†‘{diff_pct:.1f}%"
              else:
                  status = "âšª"
                  trend = f"~{diff_pct:+.1f}%"

              results.append(f"| {tool} | {tier} | {actual:,} | {baseline:,} | {trend} | {status} |")

          if results:
              print("| Tool | Tier | Actual | Baseline | Change | Status |")
              print("|------|------|-------:|----------:|-------:|:------:|")
              for r in results:
                  print(r)
          else:
              print("No token metrics found in output")
          EOF

          python3 parse_nfr.py > nfr-table.md 2>/dev/null || echo "No metrics parsed" > nfr-table.md
          cat nfr-table.md

      - name: Comment on PR
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            const pass = '${{ steps.nfr.outputs.pass }}';
            const fail = '${{ steps.nfr.outputs.fail }}';
            const exitCode = '${{ steps.nfr.outputs.exit_code }}';

            let metricsTable = 'No detailed metrics available';
            try {
              metricsTable = fs.readFileSync('nfr-table.md', 'utf8');
            } catch (e) {}

            const status = exitCode === '0' ? 'âœ… Passed' : 'âŒ Failed';
            const emoji = exitCode === '0' ? 'ğŸ‰' : 'âš ï¸';

            const body = `## ${emoji} NFR Test Results

            **Status:** ${status}

            | Metric | Count |
            |--------|------:|
            | âœ… Passed | ${pass} |
            | âŒ Failed | ${fail} |

            ### Token Budget Analysis

            ${metricsTable}

            <details>
            <summary>Legend</summary>

            - ğŸŸ¢ **Improved** - 5%+ better than baseline
            - âšª **Stable** - Within Â±5% of baseline
            - ğŸŸ¡ **Warning** - 5-10% above baseline
            - ğŸ”´ **Regression** - 10%+ above baseline (fails CI)

            </details>

            ---
            <sub>Generated by NFR workflow â€¢ [View details](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})</sub>`;

            // Find existing comment to update
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('NFR Test Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Upload NFR results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nfr-results
          path: |
            nfr-output.txt
            nfr-table.md
